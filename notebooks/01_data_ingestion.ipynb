{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a72df72-66e7-4cd3-be89-5ff6e4e83a26",
   "metadata": {},
   "source": [
    "# 01 – Data Ingestion & Schema Setup\n",
    "\n",
    "**Goals:**  \n",
    "- Connect to SQLite database with our `schema.sql`  \n",
    "- Read price CSV(s) from `data/` and compute daily returns  \n",
    "- Insert those returns into the `returns` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e307ae93-70a9-44f9-941b-ebcabf872683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine ready, database file at: ..\\data\\portfolio.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define path for the SQLite database file (in the data/ folder)\n",
    "db_path = os.path.join(\"..\", \"data\", \"portfolio.db\")\n",
    "\n",
    "# Create a SQLAlchemy engine (creates the file if it doesn't exist)\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "print(\"Engine ready, database file at:\", db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411696b-31ec-491e-b809-df1883bf69d8",
   "metadata": {},
   "source": [
    "### 1.2 Execute Schema Statements\n",
    "\n",
    "Our `sql/schema.sql` contains two statements (`DROP TABLE…;` and `CREATE TABLE…;`).  \n",
    "SQLite only executes one SQL statement at a time, so we’ll split on `;` and run each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baa1d610-2e40-4033-8d86-7cf38a8fdc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema executed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1.2 Load the DDL file\n",
    "schema_path = os.path.join(\"..\", \"sql\", \"schema.sql\")\n",
    "with open(schema_path, \"r\") as f:\n",
    "    ddl = f.read()\n",
    "\n",
    "# 1.2 Execute each statement separately\n",
    "with engine.begin() as conn:\n",
    "    for stmt in ddl.split(\";\"):\n",
    "        stmt = stmt.strip()\n",
    "        if stmt:                      # skip empty lines\n",
    "            conn.exec_driver_sql(stmt)\n",
    "print(\"Schema executed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe2262-9365-4d63-aa9f-7283701a650c",
   "metadata": {},
   "source": [
    "### 1.3 Verify Table Creation\n",
    "\n",
    "Check that the `returns` table now exists in our SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9dcf572-56bd-489c-b8c9-f0011f6f1751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in database: ['returns']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.exec_driver_sql(\n",
    "        \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    )\n",
    "    tables = [row[0] for row in result]\n",
    "print(\"Tables in database:\", tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a336857-8ab0-4620-a3bb-2abd8f693b68",
   "metadata": {},
   "source": [
    "# 02 – Read Price CSVs & Compute Daily Returns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d018940c-bacc-403d-bf52-be9c119b77b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install yfinance\n",
    "%pip install yfinance --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbfa2591-4b89-40e0-9a18-a43d98346647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14860\\3132212848.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=\"2018-01-01\", end=\"2023-12-31\", progress=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ..\\data\\GSPC.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14860\\3132212848.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=\"2018-01-01\", end=\"2023-12-31\", progress=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ..\\data\\AAPL.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14860\\3132212848.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start=\"2018-01-01\", end=\"2023-12-31\", progress=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ..\\data\\TD.csv\n"
     ]
    }
   ],
   "source": [
    "# 2.0 Fetch data via yfinance\n",
    "import os\n",
    "import yfinance as yf\n",
    "\n",
    "tickers = [\"^GSPC\", \"AAPL\", \"TD\"]\n",
    "out_dir = os.path.join(\"..\", \"data\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Download daily data\n",
    "    df = yf.download(ticker, start=\"2018-01-01\", end=\"2023-12-31\", progress=False)\n",
    "    # Save to CSV\n",
    "    filename = f\"{ticker.replace('^','')}.csv\"  # removes ^ from filename\n",
    "    csv_path = os.path.join(out_dir, filename)\n",
    "    df.to_csv(csv_path)\n",
    "    print(f\"Saved {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a3e322e-69f2-4bca-a3f1-93544be0ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\data\\\\AAPL.csv', '..\\\\data\\\\GSPC.csv', '..\\\\data\\\\TD.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print(glob.glob(os.path.join(\"..\",\"data\",\"*.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d09e6-90f8-484f-9c48-27941342242b",
   "metadata": {},
   "source": [
    "### 2.1 Load CSVs and Compute Daily Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7637b29-d373-4644-b759-2221e99e8d54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[0;32m     11\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdj Close\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     14\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ticker\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(\n\u001b[0;32m    156\u001b[0m             usecols,\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_parse_dates_presence(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_noconvert_columns()\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:243\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    233\u001b[0m missing_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m    235\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing column provided to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_dates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    248\u001b[0m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[0;32m    250\u001b[0m ]\n",
      "\u001b[1;31mValueError\u001b[0m: Missing column provided to 'parse_dates': 'Date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your data path\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "tickers = [\"GSPC\", \"AAPL\", \"TD\"]\n",
    "\n",
    "# Empty DataFrame to collect all returns\n",
    "all_returns = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    file_path = os.path.join(data_path, f\"{ticker}.csv\")\n",
    "    df = pd.read_csv(file_path, parse_dates=[\"Date\"])\n",
    "    df = df[[\"Date\", \"Adj Close\"]].copy()\n",
    "    df[\"Ticker\"] = ticker\n",
    "    df[\"Daily_Return\"] = df[\"Adj Close\"].pct_change()\n",
    "    df.dropna(inplace=True)\n",
    "    all_returns.append(df[[\"Date\", \"Ticker\", \"Daily_Return\"]])\n",
    "\n",
    "# Combine into one DataFrame\n",
    "returns_df = pd.concat(all_returns)\n",
    "returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa7bfb4-f795-47b4-b886-8c8626105974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GSPC columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "\n",
      "AAPL columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "\n",
      "TD columns: ['Price', 'Close', 'High', 'Low', 'Open', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    file_path = os.path.join(data_path, f\"{ticker}.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"\\n{ticker} columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72dce27-06e2-4ace-9a59-58f32ef5b53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
